{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from matplotlib import pyplot as plt\n",
                "from tensorflow import keras\n",
                "import cv2\n",
                "from IPython.display import Image\n",
                "import tensorflow as tf\n",
                "from tensorflow.python.ops.gen_math_ops import imag\n",
                "from IPython.display import Image, display\n",
                "from tensorflow.keras.preprocessing import image\n",
                "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
                "import tensorflow.keras.backend as K\n",
                "from keras.models import Sequential\n",
                "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
                "from tensorflow.keras.applications import imagenet_utils\n",
                "from tensorflow.keras.preprocessing.image import load_img\n",
                "import os\n",
                "from os import listdir\n",
                "from tensorflow.keras import regularizers\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
                "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
                "from keras.utils import np_utils\n",
                "from sklearn.utils import shuffle\n",
                "from sklearn.model_selection import train_test_split\n",
                "import numpy as np\n",
                "import os\n",
                "import cv2\n",
                "from random import randint\n",
                "import numpy \n",
                "from PIL import Image\n",
                "import theano\n",
                "\n",
                "import matplotlib.image as mpimg\n",
                "\n",
                "import wandb\n",
                "wandb.login()\n",
                "from wandb.keras import WandbCallback\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tf.random.set_seed(1234) #random seed to make all nn runs repeatable\n",
                "\n",
                "def all_img_loop(address,label):\n",
                "  for filename in listdir(address):\n",
                "    string_e=address+filename\n",
                "    img = cv2.imread(string_e)\n",
                "    IMG_SIZE =100\n",
                "    resized_image = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
                "    # save same img\n",
                "    cv2.imwrite(string_e, resized_image)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# all_img_loop('/home/data/data/kodeiri/ML_project/train_mix_1/Adult/','Adult')\n",
                "# all_img_loop('/home/data/data/kodeiri/ML_project/train_mix_1/Senior/','Senior')\n",
                "# all_img_loop('/home/data/data/kodeiri/ML_project/train_mix_1/Young/','Young')\n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_img_loop('/home/data/data/kodeiri/ML_project/Mix_no_yolo_aug/Adult/','Adult')\n",
                "all_img_loop('/home/data/data/kodeiri/ML_project/Mix_no_yolo_aug/Young/','Young')\n",
                "all_img_loop('/home/data/data/kodeiri/ML_project/Mix_no_yolo_aug/Senior/','Senior')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "IMG_SIZE = 100\n",
                "training = []\n",
                "test=[]\n",
                "CATEGORIES = [\"Adult\", \"Senior\", \"Young\"]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def createTrainingData(path_train,category):\n",
                "   path = path_train\n",
                "   class_num = CATEGORIES.index(category)\n",
                "   for img in os.listdir(path):\n",
                "    img_array = cv2.imread(os.path.join(path,img))\n",
                "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
                "    training.append([new_array, class_num])\n",
                "\n",
                "\n",
                "\n",
                "def createTestData(path_test,category):\n",
                "   path = path_test\n",
                "   class_num = CATEGORIES.index(category)\n",
                "   for img in os.listdir(path):\n",
                "    img_array = cv2.imread(os.path.join(path,img))\n",
                "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
                "    test.append([new_array, class_num])\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training.clear()\n",
                "createTrainingData('/home/data/data/kodeiri/ML_project/Mix_no_yolo_aug/Adult/','Adult')\n",
                "createTrainingData('/home/data/data/kodeiri/ML_project/Mix_no_yolo_aug/Senior','Senior')\n",
                "createTrainingData('/home/data/data/kodeiri/ML_project/Mix_no_yolo_aug/Young/','Young')\n",
                "print(len(training))\n",
                "#print(training[0][0])\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# training.clear()\n",
                "# createTrainingData('/home/data/data/kodeiri/ML_project/train_aug/Adult/','Adult')\n",
                "# createTrainingData('/home/data/data/kodeiri/ML_project/train_aug/Senior/','Senior')\n",
                "# createTrainingData('/home/data/data/kodeiri/ML_project/train_aug/Young/','Young')  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# test.clear()\n",
                "# createTestData('/home/data/data/kodeiri/ML_project/test_aug/Adult/','Adult')\n",
                "# createTestData('/home/data/data/kodeiri/ML_project/test_aug/Senior','Senior')\n",
                "# createTestData('/home/data/data/kodeiri/ML_project/test_aug/Young/','Young')\n",
                "# print(len(test))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "training = shuffle(training)\n",
                "\n",
                "# Assigning Labels and Features for trainig\n",
                "X_train =[]\n",
                "Y_train =[]\n",
                "for features, label in training:\n",
                "  X_train.append(features)\n",
                "  Y_train.append(label)\n",
                "X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normalising training and converting labels to categorical data\n",
                "X_train = X_train.astype('float32')\n",
                "X_train /= 255\n",
                "from keras.utils import np_utils\n",
                "print(np.shape(Y_train))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size = 0.3, random_state = 4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train=np.array(X_train)\n",
                "X_test=np.array(X_test)\n",
                "Y_train=np.array(Y_train)\n",
                "Y_test=np.array(Y_test)\n",
                "X_test, X_val = np.array_split(X_test,2)\n",
                "Y_test, Y_val = np.array_split(Y_test,2)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preparing parametars\n",
                "batch_size = 32\n",
                "nb_classes =3\n",
                "nb_epochs = 150\n",
                "img_rows, img_columns = 100, 100\n",
                "img_channel = 3\n",
                "nb_filters = 32\n",
                "nb_pool = 2\n",
                "nb_conv = 3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from keras import Sequential\n",
                "from tensorflow.keras import layers\n",
                "\n",
                "# Importing the sigmoid function from\n",
                "# Keras backend and using it\n",
                "from keras.backend import sigmoid\n",
                "  \n",
                "def swish(x, beta = 1):\n",
                "    return (x * sigmoid(beta * x))\n",
                "\n",
                "# Getting the Custom object and updating them\n",
                "from keras.utils.generic_utils import get_custom_objects\n",
                "from keras.layers import Activation\n",
                "  \n",
                "# Below in place of swish you can take any custom key for the name \n",
                "get_custom_objects().update({'swish': Activation(swish)})\n",
                "\n",
                "# Set an experiment name to group training and evaluation\n",
                "# experiment_name = wandb.util.generate_id()\n",
                "\n",
                "# # Start a run, tracking hyperparameters\n",
                "# wandb.init(\n",
                "#   project=\"dog's age prediction\",\n",
                "#   group=experiment_name,\n",
                "#   config={\n",
                "#     \"layer_1\": 64,\n",
                "#     \"activation_1\": \"leaky_relu\",\n",
                "#     \"layer_2\": 64,\n",
                "#     \"activation_2\": \"leaky_relu\",\n",
                "#     \"dropout\": 0.2,\n",
                "#     \"layer_3\": 64,\n",
                "#     \"activation_3\": \"leaky_relu\",\n",
                "#     \"layer_4\": 64,\n",
                "#     \"activation_4\": \"leaky_relu\",\n",
                "#     \"dropout\": 0.2,\n",
                "#     \"layer_5\": 64,\n",
                "#     \"activation_5\": \"leaky_relu\",\n",
                "#     \"dropout\": 0.2,\n",
                "#     \"layer_6\": 3,\n",
                "#     \"activation_6\": \"softmax\",\n",
                "#     \"optimizer\": \"adam\",\n",
                "#     \"loss\": \"sparse_categorical_crossentropy\",\n",
                "#     \"metric\": \"accuracy\",\n",
                "#     \"epoch\": 10,\n",
                "#     \"batch_size\": 16\n",
                "#   })\n",
                "# config = wandb.config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "model3=tf.keras.models.Sequential()\n",
                "model3.add(tf.keras.layers.Conv2D(64,(5,5),activation='relu',input_shape=(100,100,3), padding=\"same\"))\n",
                "model3.add(tf.keras.layers.BatchNormalization())\n",
                "model3.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
                "model3.add(tf.keras.layers.Flatten())\n",
                "model3.add(tf.keras.layers.Dense(128,activation='relu',kernel_regularizer=regularizers.l2(1e-5))) \n",
                "model3.add(tf.keras.layers.Dropout(0.3))\n",
                "model3.add(tf.keras.layers.Dense(3,activation='softmax'))  \n",
                "model3.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])\n",
                "\n",
                "trained_model = model3.fit(X_train, Y_train, batch_size = batch_size, epochs = nb_epochs, verbose = 1, validation_data = (X_val, Y_val))\n",
                "model3.summary()\n",
                "score = model3.evaluate(X_val, Y_val, verbose = 0 )\n",
                "print(\"Test Score: \", score[0])\n",
                "print(\"Test accuracy: \", score[1])\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model3.save(\"/home/data/data/kodeiri/ML_project/saved_model/my_model4\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model3 = tf.keras.models.load_model(\"/home/data/data/kodeiri/ML_project/saved_model/my_model4\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#trained_model = model3.history\n",
                "print(trained_model)\n",
                "# summarize history for accuracy\n",
                "loss_train = trained_model.history['loss']\n",
                "loss_val = trained_model.history['val_loss']\n",
                "epochs = list(range(1,nb_epochs+1))\n",
                "plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
                "plt.plot(epochs, loss_val, 'b', label='validation loss')\n",
                "plt.title('Training and Validation loss')\n",
                "plt.xlabel('Epochs')\n",
                "plt.ylabel('Loss')\n",
                "plt.legend()\n",
                "plt.show()\n",
                "#### puścić na niewytrenowanym modelu walidację\n",
                "### obserwować labele i wyjścia (może zła konwersja)\n",
                "### wyprintować run walidacyjny i porównać z ground truth"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "loss_train = trained_model.history['accuracy']\n",
                "loss_val = trained_model.history['val_accuracy']\n",
                "\n",
                "plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n",
                "plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n",
                "plt.title('Training and Validation accuracy')\n",
                "plt.xlabel('Epochs')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Y_pred =np.argmax(model3.predict(X_test), axis=-1)\n",
                "print(Y_pred)\n",
                "print(Y_test)\n",
                "\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import confusion_matrix\n",
                "cm = confusion_matrix(Y_test, Y_pred)\n",
                "sns.heatmap(cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\n",
                "#accuracy score\n",
                "from sklearn.metrics import accuracy_score\n",
                "ac=accuracy_score(Y_test, Y_pred)\n",
                "print('accuracy of the model: ',score[1])\n",
                "print(cm)\n",
                "model3.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predDecoder(prediction):\n",
                "    x = np.argmax(prediction)\n",
                "    print(\"Prediction: \" + CATEGORIES[x])\n",
                "    prob = prediction[x]\n",
                "    print(\"Confidence: \"+ str(prob))\n",
                "    return (CATEGORIES[x])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gradCAM(path,ground_truth, intensity=0.5, res=100):\n",
                "  \n",
                "  model=model3\n",
                "  intensity=0.8\n",
                "  res=100\n",
                "  #iteration_number=len(os.listdir(path))\n",
                "  iteration_number=5\n",
                "  i=0\n",
                "  cntr = 0\n",
                "  piclist=listdir(path)\n",
                "  while i < iteration_number:\n",
                "    \n",
                "    img_path=path+piclist[i]\n",
                "    img = image.load_img(img_path, target_size=(100, 100))\n",
                "\n",
                "    x = image.img_to_array(img)\n",
                "    x = np.expand_dims(x, axis=0)\n",
                "    x = x.astype(np.float32)\n",
                "    x/=255\n",
                "    preds = model.predict(x)[0]\n",
                "    predDecoder(preds)\n",
                "    print(\"Ground_truth: \"+ ground_truth)\n",
                "    if predDecoder(preds) == ground_truth:\n",
                "      cntr=cntr+1\n",
                "    # #x = preprocess_input(x)\n",
                "    conv_layer = model.get_layer(index=0)\n",
                "    heatmap_model = tf.keras.models.Model([model.inputs], [conv_layer.output, model.output])\n",
                "    \n",
                "    with tf.GradientTape() as gtape:\n",
                "        conv_output, predictions = heatmap_model(x)\n",
                "        argmax=tf.argmax(predictions[0])\n",
                "        loss = predictions[:, argmax]\n",
                "        grads = gtape.gradient(loss, conv_output)\n",
                "        pooled_grads = K.mean(grads, axis=(0,1,2))\n",
                "\n",
                "\n",
                "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1)\n",
                "    heatmap = np.maximum(heatmap, 0)\n",
                "    max_heat = np.max(heatmap)\n",
                "    if max_heat == 0:\n",
                "        max_heat = 1e-10\n",
                "    heatmap /= max_heat\n",
                "    heatmap = heatmap.squeeze()\n",
                "\n",
                "\n",
                "    img = cv2.imread(img_path)\n",
                "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
                "    heatmap = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_HOT)\n",
                "    # plt.matshow(heatmap)\n",
                "    # plt.show()\n",
                "    img = heatmap * intensity + img\n",
                "    \n",
                "    img = cv2.resize(img, (res, res))\n",
                "    img2 = cv2.cvtColor(img.astype('float32'), cv2.COLOR_BGR2RGB)\n",
                "    img2 = img2/np.amax(img2)\n",
                "    plt.imshow(img2)\n",
                "    plt.show()\n",
                "    i=i+1\n",
                "    if i == iteration_number :\n",
                "      print (\"Correct cases: \" + str(cntr))\n",
                "      print (\"Total cases: \" + str(iteration_number))\n",
                "      print (\"Prediction accuracy: \" + str(cntr/iteration_number))\n",
                "\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gradCAM('/home/data/data/kodeiri/ML_project/Mix_no_yolo_aug/Senior/',ground_truth=\"Senior\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gradCAM('/home/data/data/kodeiri/ML_project/Mix_no_yolo_aug/Adult/',ground_truth=\"Adult\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "gradCAM('/home/data/data/kodeiri/ML_project/Mix_no_yolo_aug/Young/',ground_truth=\"Young\")"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "fd29a1856f6ae0a6b1ff5c30d8a911920bc57064f70096439f4d93b631d26313"
        },
        "kernelspec": {
            "display_name": "Python 3.8.11 64-bit ('kodeiri': venv)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.11"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
